{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06c9177-fe1c-446f-b064-76f4ba9b002d",
   "metadata": {},
   "source": [
    "# Patch Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab708ee9-9fb4-4618-aa27-75e1f03ef983",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77692661-8354-4e2d-827a-4a10cd694a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import skimage\n",
    "import skimage.measure\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "from include import *\n",
    "import cv2\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "from piq import ssim, vif_p, multi_scale_ssim,psnr\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import noise\n",
    "import opensimplex\n",
    "import random\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(\"num GPUs\",torch.cuda.device_count())\n",
    "#dtype = torch.FloatTensor\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\" (c,w,h) -> (w,h,c) \"\"\"\n",
    "    image = np.transpose(tensor.numpy(), (1, 2, 0))\n",
    "    return image\n",
    "\n",
    "\n",
    "def image_to_tensor(image):\n",
    "    \"\"\" (w,h,c) -> (c,w,h) \"\"\"\n",
    "    tensor = torch.from_numpy(np.transpose(image, (2, 0, 1)))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\" Load image and normalize to [0,1]. Add 3rd dimension if original is black and white.\"\"\"\n",
    "    original_image = skimage.io.imread(image_path)\n",
    "    original_image = original_image.astype(float) / 255\n",
    "    if len(original_image.shape) == 2:\n",
    "        original_image = original_image[:, :, None]\n",
    "    return original_image\n",
    "\n",
    "\n",
    "def extract_MRI_slice(volume_path):\n",
    "\n",
    "    # Load the MRI volume\n",
    "    volume = nib.load(volume_path)\n",
    "\n",
    "    # Get the data array from the volume\n",
    "    data = volume.get_fdata()\n",
    "\n",
    "    # Get a slice from the middle of the volume\n",
    "    \n",
    "    z = data.shape[-1] // 2\n",
    "    sliced = data[..., z]\n",
    "    \n",
    "    # Normalize to [0,1]. Add 3rd dimension if original is black and white\n",
    "    \n",
    "    sliced = sliced.astype(float)\n",
    "    sliced = (sliced - np.min(sliced)) / (np.max(sliced) - np.min(sliced))\n",
    "    if len(sliced.shape) == 2:\n",
    "        sliced = sliced[:, :, None]\n",
    "        \n",
    "    return sliced\n",
    "\n",
    "\n",
    "def crop_center(image, crop_size):\n",
    "    \"\"\"\n",
    "    Crop the center of an image to a square with the specified size.\n",
    "    If the input image is smaller than the desired output size, it is padded with zeros.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : numpy.ndarray\n",
    "        The input image as a NumPy array.\n",
    "        Should have shape (height, width, channels).\n",
    "    crop_size : int\n",
    "        The desired output size (in pixels) of the square image.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The center-cropped image as a NumPy array.\n",
    "        Has shape (crop_size, crop_size, channels).\n",
    "    \"\"\"\n",
    "    # Calculate the center coordinates of the image\n",
    "    y_center = image.shape[0] // 2\n",
    "    x_center = image.shape[1] // 2\n",
    "    \n",
    "    # Calculate the start and end coordinates for the cropping\n",
    "    x_start = max(x_center - crop_size // 2, 0)\n",
    "    x_end = min(x_center + crop_size // 2, image.shape[1])\n",
    "    y_start = max(y_center - crop_size // 2, 0)\n",
    "    y_end = min(y_center + crop_size // 2, image.shape[0])\n",
    "    \n",
    "    # Crop the image and pad with zeros if necessary\n",
    "    cropped_image = np.zeros((crop_size, crop_size, image.shape[2]), dtype=image.dtype)\n",
    "    cropped_image[(crop_size - (y_end - y_start)) // 2:(crop_size - (y_end - y_start)) // 2 + (y_end - y_start),\n",
    "                  (crop_size - (x_end - x_start)) // 2:(crop_size - (x_end - x_start)) // 2 + (x_end - x_start),\n",
    "                  :] = image[y_start:y_end, x_start:x_end, :]\n",
    "    \n",
    "    # Return the result\n",
    "    return cropped_image\n",
    "\n",
    "'''\n",
    "def divide_into_patches(image, patch_size):\n",
    "    num_rows = image.shape[0] // patch_size\n",
    "    num_cols = image.shape[1] // patch_size\n",
    "    patches = {}\n",
    "    for r in range(num_rows):\n",
    "        for c in range(num_cols):\n",
    "            patch = image[r*patch_size:(r+1)*patch_size, c*patch_size:(c+1)*patch_size]\n",
    "            patches[(r, c)] = patch\n",
    "    return patches\n",
    "'''\n",
    "'''\n",
    "\n",
    "def divide_into_patches(image, patch_size):\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    patch_height, patch_width = patch_size\n",
    "\n",
    "    if image_height % patch_height != 0 or image_width % patch_width != 0:\n",
    "        print(\"Warning: The patches may leave out part of the image.\")\n",
    "\n",
    "    num_rows = image_height // patch_height\n",
    "    num_cols = image_width // patch_width\n",
    "    patches = {}\n",
    "    for r in range(num_rows):\n",
    "        for c in range(num_cols):\n",
    "            patch = image[r*patch_height:(r+1)*patch_height, c*patch_width:(c+1)*patch_width]\n",
    "            patches[(r, c)] = patch\n",
    "    return patches\n",
    "'''\n",
    "\n",
    "'''\n",
    "def merge_patches(patches):\n",
    "    # get the patch size and number of channels\n",
    "    patch_size = patches[(0, 0)].shape[0]\n",
    "    num_channels = patches[(0, 0)].shape[-1] if len(patches[(0, 0)].shape) > 2 else 1\n",
    "\n",
    "    # calculate the size of the output image\n",
    "    num_rows = max([key[0] for key in patches.keys()]) + 1\n",
    "    num_cols = max([key[1] for key in patches.keys()]) + 1\n",
    "    output_shape = (num_rows * patch_size, num_cols * patch_size, num_channels)\n",
    "\n",
    "    # create an empty output image\n",
    "    output_image = np.zeros(output_shape)\n",
    "\n",
    "    # iterate over the patches and place them in the output image\n",
    "    for key, patch in patches.items():\n",
    "        row, col = key\n",
    "        output_image[row*patch_size:(row+1)*patch_size, col*patch_size:(col+1)*patch_size] = patch\n",
    "\n",
    "    return output_image\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "def merge_patches(patches):\n",
    "    # get the patch size and number of channels\n",
    "    patch_size = patches[(0, 0)].shape[:2]\n",
    "    num_channels = patches[(0, 0)].shape[-1] if len(patches[(0, 0)].shape) > 2 else 1\n",
    "\n",
    "    # calculate the size of the output image\n",
    "    num_rows = max([key[0] for key in patches.keys()]) + 1\n",
    "    num_cols = max([key[1] for key in patches.keys()]) + 1\n",
    "    output_shape = (num_rows * patch_size[0], num_cols * patch_size[1], num_channels)\n",
    "\n",
    "    # create an empty output image\n",
    "    output_image = np.zeros(output_shape)\n",
    "\n",
    "    # iterate over the patches and place them in the output image\n",
    "    for key, patch in patches.items():\n",
    "        row, col = key\n",
    "        output_image[row*patch_size[0]:(row+1)*patch_size[0], col*patch_size[1]:(col+1)*patch_size[1]] = patch\n",
    "\n",
    "    return output_image\n",
    "\n",
    "\n",
    "def save_dictionary_to_json(dictionary, filename):\n",
    "    processed_dict = {\n",
    "        str(key): value.tolist() if isinstance(value, np.ndarray) else value\n",
    "        for key, value in dictionary.items()\n",
    "    }\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(processed_dict, file)\n",
    "        \n",
    "def load_dictionary_from_json(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        loaded_dict = json.load(file)\n",
    "\n",
    "    recovered_dict = {}\n",
    "    for key_str, value in loaded_dict.items():\n",
    "        key_tuple = ast.literal_eval(key_str)\n",
    "        if isinstance(value, list):\n",
    "            value = np.array(value)\n",
    "        recovered_dict[key_tuple] = value\n",
    "\n",
    "    return recovered_dict\n",
    "\n",
    "def rep_error_deep_decoder(img_np,k=128,convert2ycbcr=False):\n",
    "    '''\n",
    "    mse obtained by representing img_np with the deep decoder\n",
    "    '''\n",
    "    \n",
    "    output_depth = img_np.shape[0]\n",
    "    if output_depth == 3 and convert2ycbcr:\n",
    "        img = rgb2ycbcr(img_np)\n",
    "    else:\n",
    "        img = img_np\n",
    "    \n",
    "    #CAUTION\n",
    "    img_var = np_to_var(img).type(dtype)\n",
    "    \n",
    "    #img_var = img\n",
    "    \n",
    "    num_channels = [k]*5\n",
    "    print(num_channels)\n",
    "    net = decodernw(output_depth,num_channels_up=num_channels,upsample_first = True).type(dtype)\n",
    "    rnd = 500\n",
    "    numit = 20000\n",
    "    rn = 0.005\n",
    "    mse_n, mse_t, ni, net = fit( num_channels=num_channels,\n",
    "                        reg_noise_std=rn,\n",
    "                        reg_noise_decayevery = rnd,\n",
    "                        num_iter=numit,\n",
    "                        LR=0.005,\n",
    "                        img_noisy_var=img_var,\n",
    "                        net=net,\n",
    "                        img_clean_var=img_var,\n",
    "                        find_best=True,\n",
    "                               )\n",
    "    out_img = net(ni.type(dtype)).data.cpu().numpy()[0]\n",
    "    if output_depth == 3 and convert2ycbcr:\n",
    "        out_img = ycbcr2rgb(out_img)\n",
    "    return psnr(out_img,img_np), out_img, num_param(net)\n",
    "\n",
    "def myimgshow(plt,img):\n",
    "    if(img.shape[0] == 1):\n",
    "        plt.imshow(np.clip(img[0],0,1),cmap='Greys',interpolation='none')\n",
    "    else:\n",
    "        plt.imshow(np.clip(img.transpose(1, 2, 0),0,1),interpolation='none')\n",
    "\n",
    "def comparison(img_np, k, convert2ycbcr=False):\n",
    "    # compute representations\n",
    "    psnrv, out_img_np, nparms = rep_error_deep_decoder(img_np, k=k, convert2ycbcr=convert2ycbcr)\n",
    "    \n",
    "    print(\"Compression factor: \", np.prod(img_np.shape) / nparms)\n",
    "    \n",
    "    # plot results\n",
    "    fig = plt.figure(figsize=(15, 15)) # create a 5 x 5 figure \n",
    "    \n",
    "    ax1 = fig.add_subplot(121)\n",
    "    myimgshow(ax1, img_np) \n",
    "    ax1.set_title('Original image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = fig.add_subplot(122)\n",
    "    myimgshow(ax2, out_img_np)\n",
    "    ax2.set_title(\"Deep-Decoder representation, PSNR: %.2f\" % psnrv)\n",
    "    ax2.axis('off')\n",
    "    # save_np_img(img_np,\"exp_comp_orig.png\")\n",
    "    # save_np_img(out_img_np,\"exp_comp_dd.png\")\n",
    "\n",
    "    plt.axis('off')\n",
    "    fig.show()\n",
    "    plt.show()\n",
    "    return out_img_np\n",
    "\n",
    "\n",
    "#Restore a dictonary from a tensor of processed patches\n",
    "def restore_from_tensor(batch_tensor):\n",
    "    patch_size = batch_tensor.shape[-2:]  # Assuming all patches have the same size\n",
    "    num_rows = batch_tensor.shape[0]\n",
    "    num_cols = batch_tensor.shape[1]\n",
    "\n",
    "    restored_patches = {}\n",
    "    for r in range(num_rows):\n",
    "        for c in range(num_cols):\n",
    "            patch_tensor = batch_tensor[r, c].cpu()            \n",
    "            patch_data = patch_tensor.numpy()  # Convert tensor to NumPy array\n",
    "\n",
    "            if len(patch_data.shape) == 2:  # Single-channel patch (grayscale)\n",
    "                patch_data = np.expand_dims(patch_data, axis=2)  # Add channel dimension\n",
    "\n",
    "            patch_data = np.transpose(patch_data, (0, 1, 2))  # Transpose to (height, width, channels)\n",
    "\n",
    "            patch_key = (r, c)\n",
    "            restored_patches[patch_key] = patch_data\n",
    "\n",
    "    return restored_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de07251-6bbd-4575-ba67-56c2317aa6cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bde44-7823-4d44-a502-5f27a137662c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def divide_into_patches(image, patch_size):\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    patch_height, patch_width = patch_size\n",
    "\n",
    "    num_rows = image_height // patch_height\n",
    "    num_cols = image_width // patch_width\n",
    "\n",
    "    new_height = num_rows * patch_height\n",
    "    new_width = num_cols * patch_width\n",
    "\n",
    "    pad_height = new_height - image_height\n",
    "    pad_width = new_width - image_width\n",
    "    \n",
    "    if image_height // patch_height != image_height / patch_height or image_width // patch_width != image_width / patch_width:\n",
    "        print ('Caution, not perfect division')\n",
    "\n",
    "    patches = {}\n",
    "    for r in range(num_rows):\n",
    "        for c in range(num_cols):\n",
    "            patch = image[r * patch_height:(r + 1) * patch_height, c * patch_width:(c + 1) * patch_width]\n",
    "            patches[(r, c)] = patch\n",
    "    return patches\n",
    "\n",
    "'''\n",
    "    if pad_height > 0 or pad_width > 0:\n",
    "        print(\"Padding the image to ensure patch size compatibility.\")\n",
    "        print(\"Padded image dimensions:\", new_height, new_width)\n",
    "\n",
    "        # Pad the image with zeros\n",
    "        image = np.pad(image, ((0, pad_height), (0, pad_width), (0, 0)), mode='constant')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc1a53-b8ec-4234-b895-58065e7c083a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sliced_patches = divide_into_patches(sliced, (sliced.shape[0]//3, sliced.shape[1]//3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3a2b8-32cf-4ba5-aa03-2160d40f33ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sliced = extract_MRI_slice('test_data/100206_MPR1.nii.gz')\n",
    "print(\"Size of the original image: \", sliced.shape)\n",
    "\n",
    "# Create a figure with a specific size\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plotting the first image\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.imshow(sliced)\n",
    "ax1.set_title(\"Original Slice\")\n",
    "\n",
    "\n",
    "sliced_crop = crop_center(sliced, 256)\n",
    "\n",
    "print(\"Size of the cropped image: \", sliced_crop.shape)\n",
    "\n",
    "\n",
    "# Plotting the cropped image\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.imshow(sliced_crop)\n",
    "ax2.set_title(\"Cropped Slice\")\n",
    "\n",
    "# Adjust spacing and align the subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95950e-718c-4b07-9639-fec478e326e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this Notebook this is only for the sake of initializing the fitting configuration\n",
    "sliced_patches = divide_into_patches(sliced, (sliced.shape[0]//3, sliced.shape[1]//3))\n",
    "\n",
    "batch_np = np.array(list(sliced_patches.values())).squeeze()[:,None,:,:]\n",
    "target_tensor = torch.from_numpy(batch_np).type(dtype)\n",
    "print('Target tensor shape: ', target_tensor.shape)\n",
    "print()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, len(batch_np), figsize=(12, 4))  # Create a single row of subplots\n",
    "\n",
    "for i, image in enumerate(batch_np):\n",
    "    axes[i].imshow(image.squeeze())  # Assign each image to a subplot\n",
    "    axes[i].axis('off')  # Turn off axis labels\n",
    "    axes[i].set_title(f'{i+1}')  # Set subplot title if desired\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "original_image = sliced_patches[0,0] \n",
    "# Just to define a variable to extract the size in the 'fit_model_configuration.input_shape'\n",
    "# Should be changed to make it automatic?\n",
    "target_image=np.array(batch_np.tolist())\n",
    "\n",
    "print('Target image shape: ',target_image.shape)\n",
    "\n",
    "num_patches = len(sliced_patches)\n",
    "image_ids = list(range(num_patches))\n",
    "number_of_images_fitted = len(image_ids)\n",
    "\n",
    "\n",
    "print('Number of patches: ', num_patches)\n",
    "print('Image ids: ', image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f7609-5727-4914-aca0-687d6e596269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sliced_patches[0,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7429fef-31f4-4fd8-a487-203216a5a0ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fitting Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735a6f2-53fa-4ed7-adfb-83b0045fce0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define fitting configuration object controlling all relevant parameters\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class FittingConfiguration:\n",
    "    def __init__(self):\n",
    "        self.image_dimensions = None\n",
    "        self.result_path = None\n",
    "\n",
    "        # Architecture Parameters\n",
    "        self.model_type = 'deep'\n",
    "        self.input_shape = None\n",
    "        self.number_of_layers = 5\n",
    "        self.number_of_hidden_channels = 32\n",
    "\n",
    "        # Fitting Parameters\n",
    "        self.number_of_iterations = 1500\n",
    "        self.number_of_runs = 3\n",
    "        self.learning_rate = 0.1\n",
    "        self.convergence_check_length = None\n",
    "        self.log_frequency = 100\n",
    "        self.find_best = True\n",
    "        self.save_losses = False\n",
    "        self.constant_input = True\n",
    "        self.compactness_parameter = 0.9\n",
    "        blur_factor = 0.0001\n",
    "        self.input_blur = 0\n",
    "        self.lr_schedule_factor = 0.8\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.data_type = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            self.data_type = torch.FloatTensor\n",
    "\n",
    "    def __str__(self):\n",
    "        dictionary = self.__dict__\n",
    "        result = \"\"\n",
    "        for key in dictionary:\n",
    "            result += key + \": \" + str(dictionary[key]) + \"  \" + os.linesep\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738d4be-5cfb-43be-84b8-834f4e33a75c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3D Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3871428-77f3-495c-883d-229de1bf4742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "#from fitting.helpers import generate_random_variants\n",
    "from utils.image_helpers import image_to_tensor, tensor_to_image\n",
    "\n",
    "# The dropout reduction can be commented out in the Fitter.fit() function\n",
    "DROPOUT_REDUCTION = {1: 0.8, 350: 0.4, 700: 0.2, 1050: 0.1, 1400: 0.05, 1750: 0.025, 2100: 0.0125, 2500: 0.0}\n",
    "\n",
    "\n",
    "def create_fitter_from_configuration(configuration):\n",
    "    fitter = Fitter(number_of_iterations=configuration.number_of_iterations,\n",
    "                    learning_rate=configuration.learning_rate,\n",
    "                    convergence_check_length=configuration.convergence_check_length,\n",
    "                    log_frequency=configuration.log_frequency,\n",
    "                    find_best=configuration.find_best,\n",
    "                    data_type=configuration.data_type,\n",
    "                    save_losses=configuration.save_losses,\n",
    "                    constant_fixed_input=configuration.constant_input,\n",
    "                    loss_weight=configuration.loss_weight,\n",
    "                    input_blur=configuration.input_blur,\n",
    "                    lr_schedule_factor=configuration.lr_schedule_factor)\n",
    "    return fitter\n",
    "\n",
    "\n",
    "class Fitter:\n",
    "    def __init__(self, number_of_iterations, learning_rate=0.01, convergence_check_length=None, log_frequency=10,\n",
    "                 find_best=False, data_type=torch.FloatTensor, save_losses=False, constant_fixed_input=False,\n",
    "                 loss_weight=0.95, input_blur=0, lr_schedule_factor=0.5):\n",
    "        self.loss_function = torch.nn.MSELoss().type(data_type)\n",
    "        self.number_of_iterations = number_of_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.convergence_check_length = convergence_check_length\n",
    "        self.log_frequency = log_frequency\n",
    "        self.find_best = find_best\n",
    "        self.data_type = data_type\n",
    "        self.save_losses = save_losses\n",
    "        self.constant_fixed_input = constant_fixed_input\n",
    "        self.fixed_net_input = None\n",
    "        self.model = None\n",
    "        self.save_steps = False\n",
    "        self.loss_weight = loss_weight\n",
    "        self.input_blur = input_blur\n",
    "        self.lr_schedule_factor = lr_schedule_factor\n",
    "\n",
    "    def __call__(self, model, original_image, fixed_net_input, log_prefix=None, loss_mask=None):\n",
    "        if self.model is None:\n",
    "            self.model = model.type(self.data_type)\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 400, self.lr_schedule_factor)\n",
    "            self.best_model = copy.deepcopy(self.model)\n",
    "            self.best_model_step = 0\n",
    "            self.best_model_loss = 1000\n",
    "            self.step_images = []\n",
    "            self.step_counter = 0\n",
    "\n",
    "        self.fixed_net_input = fixed_net_input\n",
    "        self.noisy_image = original_image\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(list(model.parameters()), lr=self.learning_rate) #+list(self.fixed_net_input)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 400, self.lr_schedule_factor)\n",
    "\n",
    "        self.target_image = None\n",
    "\n",
    "        if loss_mask is None:\n",
    "            self.loss_mask = torch.ones(self.noisy_image.shape).type(self.data_type)\n",
    "        else:\n",
    "            self.loss_mask = torch.from_numpy(loss_mask).squeeze().type(self.data_type)[None, :, :].type(self.data_type)\n",
    "\n",
    "        if self.save_losses:\n",
    "            self.losses_wrt_noisy = []\n",
    "            self.losses_wrt_target = []\n",
    "        self.current_loss_wrt_noisy = 1000\n",
    "        self.current_loss_wrt_target = 1000\n",
    "        if log_prefix is None:\n",
    "            self.log_prefix = ''\n",
    "        else:\n",
    "            self.log_prefix = log_prefix\n",
    "        \n",
    "\n",
    "        self.fit()\n",
    "\n",
    "    def fit(self):\n",
    "        while self.has_not_converged() and self.step_counter < self.number_of_iterations:\n",
    "            def closure():\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(self.fixed_net_input)\n",
    "                loss = self.loss_function(self.noisy_image, output)\n",
    "                loss.backward()\n",
    "                self.update_loss_metrics_and_best_model(loss, output)\n",
    "                self.scheduler.step()\n",
    "            self.optimizer.step(closure)\n",
    "            self.step_counter += 1\n",
    "\n",
    "            if self.should_log():\n",
    "                self.log()\n",
    "\n",
    "            if self.step_counter in DROPOUT_REDUCTION.keys():\n",
    "                #print(f'Reduced to {DROPOUT_REDUCTION[self.step_counter]}.')\n",
    "                for module in self.model.module_list[2:]:\n",
    "                    if type(module) == torch.nn.modules.dropout.Dropout:\n",
    "                        module.p = 0#DROPOUT_REDUCTION[self.step_counter]\n",
    "\n",
    "    def has_not_converged(self):\n",
    "        if self.convergence_check_length is None:\n",
    "            return True\n",
    "        elif self.step_counter < self.convergence_check_length:\n",
    "            return True\n",
    "        else:\n",
    "            if self.best_model_step < self.step_counter - self.convergence_check_length:\n",
    "                print(self.log_prefix + f'Converged at step {self.step_counter}.' + ' ' * 50, end='\\r')\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def update_loss_metrics_and_best_model(self, current_loss_wrt_noisy, current_output):\n",
    "        self.model.eval()\n",
    "        self.current_loss_wrt_noisy = current_loss_wrt_noisy.data\n",
    "\n",
    "        if self.save_losses:\n",
    "            self.losses_wrt_noisy.append(self.current_loss_wrt_noisy)\n",
    "\n",
    "        if self.target_image is not None:\n",
    "            current_loss_wrt_target = self.loss_function(current_output, self.target_image)\n",
    "            self.current_loss_wrt_target = current_loss_wrt_target.data\n",
    "            if self.save_losses:\n",
    "                self.losses_wrt_target.append(self.current_loss_wrt_target.data)\n",
    "\n",
    "        if self.find_best:\n",
    "            if self.step_counter > 0:\n",
    "                if self.best_model_loss > 1.005 * current_loss_wrt_noisy.data:\n",
    "                    self.best_model = copy.deepcopy(self.model)\n",
    "                    self.best_model_step = self.step_counter\n",
    "                    self.best_model_loss = current_loss_wrt_noisy.data\n",
    "        elif self.step_counter == self.number_of_iterations - 1:\n",
    "            self.best_model = copy.deepcopy(self.model)\n",
    "            for params, best_params in zip(self.model.named_parameters(), self.best_model.named_parameters()):\n",
    "                best_params[1].grad = params[1].grad\n",
    "            self.best_model_step = self.step_counter\n",
    "            self.best_model_loss = current_loss_wrt_noisy.data\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "    def should_log(self):\n",
    "        if self.step_counter % self.log_frequency == 0:\n",
    "            return True\n",
    "        elif self.step_counter == self.number_of_iterations:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def log(self):\n",
    "        log_string = self.log_prefix\n",
    "        log_string += f\"Step: {self.step_counter:05d}\"\n",
    "        log_string += \", \"\n",
    "        log_string += f\"Loss: {self.current_loss_wrt_noisy:.6f}\"\n",
    "        if self.target_image is not None:\n",
    "            log_string += \", \"\n",
    "            log_string += f\"Target Loss: {self.current_loss_wrt_target:.6f}\"\n",
    "        if self.find_best:\n",
    "            log_string += ', '\n",
    "            log_string += f'Minimum Loss at: {self.best_model_step} with {self.best_model_loss:.6f}'\n",
    "        print(log_string, end='\\r')\n",
    "\n",
    "    def get_best_image(self):\n",
    "        return self.best_model(self.fixed_net_input).detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523fb44f-e703-48bf-b73e-4e38109f48d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f1806-44ea-40a0-9d29-4abd4a6e448f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepModule(nn.Module):\n",
    "    def __init__(self, number_of_input_channels, number_of_output_channels, upsample_size,\n",
    "                 upsample_mode='bilinear', bn=True):\n",
    "        self.bn = bn\n",
    "        if number_of_input_channels is None:\n",
    "            number_of_input_channels = number_of_output_channels\n",
    "        super(DeepModule, self).__init__()\n",
    "        self.convolution_layer = nn.Conv2d(in_channels=number_of_input_channels, out_channels=number_of_output_channels,\n",
    "                                           kernel_size=1, stride=1, bias=False)\n",
    "        self.upsample_layer = nn.Upsample(size=upsample_size, mode=upsample_mode, align_corners=True)\n",
    "        self.activation_layer = nn.ReLU()\n",
    "        if self.bn:\n",
    "            # In the original paper they used batch norm. They only used batches of size one. \n",
    "            # Hence, in this setup we had to change it to InstanceNorm s.t. the normalization happens channelwise.\n",
    "            self.batch_normalization = nn.InstanceNorm2d(num_features=number_of_output_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "    def forward(self, module_input):\n",
    "        x = self.convolution_layer(module_input)\n",
    "        x = self.upsample_layer(x)\n",
    "        x = self.activation_layer(x)\n",
    "        if self.bn:\n",
    "            x = self.batch_normalization(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepDecoder(nn.Module):\n",
    "    def __init__(self, input_shape, image_dimensions, number_of_layers, number_of_hidden_channels, upsample_sizes=None):\n",
    "        super(DeepDecoder, self).__init__()\n",
    "        self.input_shape = [1, number_of_hidden_channels] + input_shape\n",
    "        self.image_dimensions = image_dimensions[:2]\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.number_of_hidden_channels = number_of_hidden_channels\n",
    "        self.number_of_output_channels = image_dimensions[2]\n",
    "\n",
    "        # Initialize Module List to be filled with Module consisting of layers\n",
    "        self.module_list = nn.ModuleList()\n",
    "\n",
    "        # Fill Module List with Modules consisting of Reflection, Convolution, Upsampling, ReLU and Batch Normalization\n",
    "        if number_of_layers > 0:\n",
    "            if upsample_sizes is None:\n",
    "                upsample_sizes = calculate_upsample_sizes(input_shape, self.image_dimensions, number_of_layers)\n",
    "            for layer_index, upsample_size in enumerate(upsample_sizes):\n",
    "                self.module_list.append(DeepModule(number_of_hidden_channels, number_of_hidden_channels, upsample_size, bn=True))\n",
    "\n",
    "        # Add final module\n",
    "        self.module_list.append(nn.Conv2d(in_channels=number_of_hidden_channels,\n",
    "                                          out_channels=self.number_of_output_channels,\n",
    "                                          kernel_size=1,\n",
    "                                          stride=1,\n",
    "                                          bias=False))\n",
    "        self.module_list.append(nn.Sigmoid())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        return self.input_shape\n",
    "\n",
    "    def __str__(self):\n",
    "        output_string = \"Deep Decoder \"\n",
    "        output_string += str(self.input_shape[-2:])\n",
    "        output_string += \", \"\n",
    "        output_string += str(self.number_of_layers)\n",
    "        output_string += \", \"\n",
    "        output_string += str(self.number_of_hidden_channels)\n",
    "        return output_string\n",
    "\n",
    "    def get_model_parameters(self):\n",
    "        return ['deep', self.input_shape[-2:], self.number_of_layers, self.number_of_hidden_channels]\n",
    "\n",
    "\n",
    "def calculate_upsample_sizes(input_shape, output_shape, number_of_layers):\n",
    "    scale = (np.array(output_shape) / np.array(input_shape)) ** (1 / number_of_layers)\n",
    "    upsample_sizes = [np.ceil(np.array(input_shape) * (scale ** n)).astype(int).tolist() for n in\n",
    "                      range(1, number_of_layers)] + [output_shape]\n",
    "    return upsample_sizes\n",
    "\n",
    "def create_model_from_configuration(fit_model_configuration):\n",
    "    model = DeepDecoder(input_shape=fit_model_configuration.input_shape,\n",
    "                        image_dimensions=fit_model_configuration.image_dimensions,\n",
    "                        number_of_layers=fit_model_configuration.number_of_layers,\n",
    "                        number_of_hidden_channels=fit_model_configuration.number_of_hidden_channels)\n",
    "    return model\n",
    "\n",
    "\n",
    "class DecoderEnsemble:\n",
    "    def __init__(self):\n",
    "        self.decoders = []\n",
    "        self.start_inputs = []\n",
    "        self.input_deltas = []\n",
    "    \n",
    "    def add_decoder(self, decoder, start_input, input_delta):\n",
    "        self.decoders.append(decoder)\n",
    "        self.start_inputs.append(start_input)\n",
    "        self.input_deltas.append(input_delta)\n",
    "        \n",
    "    def evaluate(self, z, num_dds=None):\n",
    "        preds = []\n",
    "        for decoder, start_input, input_delta in zip(self.decoders, self.start_inputs, self.input_deltas):\n",
    "            input_tensor = start_input + z*input_delta*STEP_SIZE\n",
    "            pred = decoder(input_tensor.type(dtype)).cpu().detach().numpy()\n",
    "            preds.append(pred.squeeze())\n",
    "        return np.mean(preds[:num_dds], axis=0)\n",
    "    \n",
    "    def predict_volume(self, z_1, z_2, num_frames=None, num_dds=None):\n",
    "        assert isinstance(z_1, int) and isinstance(z_2, int)\n",
    "        assert z_2 > z_1\n",
    "        if num_dds is None:\n",
    "            num_dds = len(self.decoders)\n",
    "        if num_frames is None:\n",
    "            num_frames = z_2 - z_1\n",
    "        frames = []\n",
    "        for z in np.linspace(z_1, z_2, num_frames):\n",
    "            frames.append(self.evaluate(z, num_dds))\n",
    "        return np.transpose(np.array(frames), [1,2,0])\n",
    "    \n",
    "    def get_features(self, z):\n",
    "        feature_maps = []\n",
    "        for decoder, start_input, input_delta in zip(self.decoders, self.start_inputs, self.input_deltas):\n",
    "            input_tensor = start_input + z*input_delta*STEP_SIZE\n",
    "            \n",
    "            activation = {}\n",
    "            def get_activation(name):\n",
    "                def hook(model, input, output):\n",
    "                    activation[name] = output.detach()\n",
    "                return hook\n",
    "\n",
    "            hook = decoder.module_list[-3].register_forward_hook(get_activation(f'act'))\n",
    "            _ = decoder(input_tensor.type(dtype))\n",
    "\n",
    "            act_maps = list(activation['act'].squeeze().cpu().numpy())\n",
    "            weights = list(decoder.module_list[-2].weight.detach().squeeze().cpu().numpy())\n",
    "            feature_maps.extend([a*w for a,w in zip(act_maps, weights)])\n",
    "            hook.remove()\n",
    "        return feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2de497-59c1-49b1-bc8f-6dea171a57d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fitting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade3771-d53b-4681-8f68-026db3e91cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fit_model_configuration = FittingConfiguration()\n",
    "\n",
    "fit_model_configuration.model_type = 'deep'\n",
    "\n",
    "fit_model_configuration.number_of_hidden_channels = int(64)\n",
    "fit_model_configuration.number_of_layers = 5\n",
    "fit_model_configuration.number_of_iterations = 2000\n",
    "\n",
    "fit_model_configuration.log_frequency = 50\n",
    "fit_model_configuration.convergence_check_length = None\n",
    "fit_model_configuration.result_path = None\n",
    "fit_model_configuration.save_losses = True\n",
    "fit_model_configuration.find_best = True\n",
    "fit_model_configuration.constant_input = True\n",
    "fit_model_configuration.input_shape = [original_image.shape[0]//5, original_image.shape[1]//5]\n",
    "fit_model_configuration.image_dimensions =  [target_tensor.shape[2], target_tensor.shape[3], target_tensor.shape[1]]\n",
    "fit_model_configuration.number_of_runs = 5\n",
    "\n",
    "\n",
    "fit_model_configuration.lr_schedule_factor = 0.6\n",
    "fit_model_configuration.loss_weight = 1.0#0.95\n",
    "\n",
    "print(fit_model_configuration)\n",
    "\n",
    "conv_maps = []#np.zeros((0, original_image.shape[0], original_image.shape[0]))\n",
    "preds = []\n",
    "weights = np.zeros((target_image.shape[2], 0))\n",
    "\n",
    "decoder = create_model_from_configuration(fit_model_configuration)\n",
    "param_num = np.sum([weight.numel() for weight in decoder.parameters()])\n",
    "print(f'Number of Parameters: {param_num}')\n",
    "#print(f'Compression per Decoder: {param_num*32/volume.size*100:0.2f}%') # param_num * 32 because float32\n",
    "#print(f'Compression Ensemble: {param_num*32/volume.size*100*fit_model_configuration.number_of_runs:0.2f}%')\n",
    "#print('Compression rates given in: Weights [bits] / Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0481cdf-ff1c-4885-9a32-0213121e1d80",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Input Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b885ee8-46a7-4e29-b037-d362fad4e6de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_random_variants(number_of_hidden_channels, input_size, blur=0):\n",
    "    input_images = []\n",
    "    gen_counter = 0\n",
    "\n",
    "    while len(input_images) < number_of_hidden_channels and gen_counter < number_of_hidden_channels * 5:\n",
    "        noise = np.random.uniform(low=0, high=1, size=[*input_size])\n",
    "        if blur == 0:\n",
    "            convolution = noise\n",
    "        else:\n",
    "            convolution = cv2.GaussianBlur(noise, (blur, blur), 0)\n",
    "            convolution = convolution - np.min(convolution)\n",
    "            convolution = convolution / np.max(convolution)\n",
    "            convolution = 2 * convolution - 1\n",
    "\n",
    "        input_images.append(convolution)\n",
    "\n",
    "        gen_counter += 1\n",
    "    for _ in range(number_of_hidden_channels - len(input_images)):\n",
    "        input_images.append(np.zeros([input_size] * 2))\n",
    "    return input_images\n",
    "\n",
    "STEP_SIZE = 0.08\n",
    "def generate_inputs(num, step_size=STEP_SIZE, blur=3):\n",
    "    \"\"\"\n",
    "    Generate two random noise fields with the function generate_random_variants and linearly interpolate between them to generate \"3D\" input such that each\n",
    "    input slice maps to one slice of the target volume.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = []\n",
    "    fixed_net_input = torch.zeros(size=[number_of_images_fitted,fit_model_configuration.number_of_hidden_channels]+fit_model_configuration.input_shape)\n",
    "    start_input = torch.from_numpy(np.array(generate_random_variants(fit_model_configuration.number_of_hidden_channels, fit_model_configuration.input_shape, blur))).unsqueeze(0)\n",
    "    end_input = torch.from_numpy(np.array(generate_random_variants(fit_model_configuration.number_of_hidden_channels, fit_model_configuration.input_shape, blur))).unsqueeze(0) + 0.1\n",
    "    delta = (end_input-start_input)\n",
    "\n",
    "    for z in range(num):\n",
    "        fixed_net_input[z] = start_input + z*delta*step_size\n",
    "    fixed_net_input = fixed_net_input.type(dtype)\n",
    "    \n",
    "    return fixed_net_input, start_input, delta\n",
    "\n",
    "\n",
    "\n",
    "fixed_net_input, start_input, delta = generate_inputs(len(image_ids))\n",
    "print(fixed_net_input.shape)\n",
    "\n",
    "def divide_samples_into_patches(samples, patch_size):\n",
    "    num_samples = samples.shape[0]\n",
    "    patch_height, patch_width = patch_size\n",
    "    num_rows = samples.shape[1] // patch_height\n",
    "    num_cols = samples.shape[2] // patch_width\n",
    "    patches = np.empty((num_rows * num_cols, num_samples, patch_height, patch_width))\n",
    "\n",
    "    for s in range(num_samples):\n",
    "        image = samples[s]\n",
    "        patch_index = 0\n",
    "        for r in range(num_rows):\n",
    "            for c in range(num_cols):\n",
    "                patch = image[r*patch_height:(r+1)*patch_height, c*patch_width:(c+1)*patch_width]\n",
    "                patches[patch_index, s] = patch\n",
    "                patch_index += 1\n",
    "\n",
    "    return patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f73f8-b13e-414f-b54c-7c59bcef525d",
   "metadata": {},
   "source": [
    "# Perlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e51b7-f33a-4915-aae1-14c78c2554e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_simplex_noise_patches(size, number_of_channels, seed):\n",
    "    simplex_noise = np.empty((number_of_channels, size[0], size[1]))\n",
    "\n",
    "    for channel in range(number_of_channels):\n",
    "        noise_generator = opensimplex.OpenSimplex(seed=seed + channel)\n",
    "        for i in range(size[0]):\n",
    "            for j in range(size[1]):\n",
    "                value = noise_generator.noise3(i, j, channel)\n",
    "                mapped_value = (value + 1) / 2  # Map the value to the range [0, 1]\n",
    "                simplex_noise[channel, i, j] = mapped_value\n",
    "\n",
    "    return simplex_noise\n",
    "\n",
    "# Example usage\n",
    "seed = np.random.randint(1, 10**3)\n",
    "print('Seed:', seed)\n",
    "\n",
    "\n",
    "number_of_hidden_channels = fit_model_configuration.number_of_hidden_channels\n",
    "input_size = fit_model_configuration.input_shape\n",
    "        \n",
    "simplex_noise = generate_simplex_noise_patches(size = (2*input_size[0], 2*input_size[1]),\n",
    "                                               number_of_channels = number_of_hidden_channels, \n",
    "                                               seed = np.random.randint(1, 10**3))\n",
    "\n",
    "print(simplex_noise.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38901eb1-272e-4792-baf6-3f06f48400a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grid Search Fitting Patches with 5 Decoders (No Blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cee74b67-235d-4a21-b080-892b39b8bf4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caution, not perfect division\n",
      "Original image shape:  (85, 106, 1)\n",
      "Target tensor shape:  torch.Size([9, 1, 85, 106])\n",
      "\n",
      "Run 0:\n",
      "number_of_hidden_channels: 8\n",
      "input_shape: [2, 3]\n",
      "number_of_layers: 4\n",
      "number_of_iterations: 2000\n",
      "\n",
      "Decoder  0\n",
      "Step: 02000, Loss: 0.012465, Minimum Loss at: 614 with 0.011572\n",
      "Number of Parameters: 328\n",
      "\n",
      "PSNR:  19.491214752197266\n",
      "\n",
      "Decoder  1\n",
      "Step: 00200, Loss: 0.015059, Minimum Loss at: 170 with 0.014860\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define target image\n",
    "if 'original_image' in locals():\n",
    "    del original_image\n",
    "\n",
    "if 'target_tensor' in locals():\n",
    "    del target_tensor\n",
    "# Original image must be (H,W,C)\n",
    "# Target tensor must be (Batch_size, C, H, W)\n",
    "\n",
    "\n",
    "sliced =  extract_MRI_slice('test_data/100206_MPR1.nii.gz')\n",
    "sliced_patches = divide_into_patches(sliced, (sliced.shape[0]//3, sliced.shape[1]//3))\n",
    "original_image = sliced_patches[0,0]\n",
    "batch_np = np.array(list(sliced_patches.values())).squeeze()[:,None,:,:]\n",
    "target_tensor = torch.from_numpy(batch_np).type(dtype)\n",
    "\n",
    "print('Original image shape: ', original_image.shape)\n",
    "print('Target tensor shape: ', target_tensor.shape)\n",
    "print()\n",
    "\n",
    "# Define candidate values for each parameter\n",
    "fit_model_configuration.image_dimensions = [target_tensor.shape[2], target_tensor.shape[3], target_tensor.shape[1]]\n",
    "number_of_hidden_channels_values = [8, 16, 32, 64, int(64*1.5), 64*2, 64*3]\n",
    "input_shape_values = [[original_image.shape[0] // 2**5, original_image.shape[1] // 2**5],\n",
    "                      [original_image.shape[0] // 2**4, original_image.shape[1] // 2**4],\n",
    "                      [original_image.shape[0] // 2**3, original_image.shape[1] // 2**3]]\n",
    "number_of_layers_values = [4, 5, 6]\n",
    "number_of_iterations_values = [2000, 5000, 7000]\n",
    "\n",
    "# Initialize variables\n",
    "best_psnr = 0\n",
    "best_ssim = 0\n",
    "best_mssim = 0\n",
    "best_vif = 0\n",
    "\n",
    "best_psnr_configuration = None\n",
    "best_ssim_configuration = None\n",
    "best_mssim_configuration = None\n",
    "best_vif_configuration = None\n",
    "\n",
    "with open('grid_search_results_patches_9_5DD_blur_0.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Run', 'Number_of_hidden_channels', 'Input_shape', 'Number_of_layers', 'Number_of_iterations',\n",
    "                     'PSNR','PSNR Variance', 'SSIM', 'VIF', 'Number_of_parameters', 'Execution_time'])\n",
    "\n",
    "    for count, config in enumerate(itertools.product(number_of_hidden_channels_values, input_shape_values,\n",
    "                                                     number_of_layers_values, number_of_iterations_values)):\n",
    "        \n",
    "                \n",
    "        decoder = None\n",
    "        fitter = None\n",
    "        output = None\n",
    "\n",
    "        fit_model_configuration.number_of_hidden_channels = config[0]\n",
    "        fit_model_configuration.input_shape = config[1]\n",
    "        fit_model_configuration.number_of_layers = config[2]\n",
    "        fit_model_configuration.number_of_iterations = config[3]\n",
    "\n",
    "        print(f'Run {count}:')\n",
    "        print('number_of_hidden_channels:', fit_model_configuration.number_of_hidden_channels)\n",
    "        print('input_shape:', fit_model_configuration.input_shape)\n",
    "        print('number_of_layers:', fit_model_configuration.number_of_layers)\n",
    "        print('number_of_iterations:', fit_model_configuration.number_of_iterations)\n",
    "        print()\n",
    "\n",
    "        number_of_hidden_channels = fit_model_configuration.number_of_hidden_channels\n",
    "        input_size = fit_model_configuration.input_shape\n",
    "        blur = 0\n",
    "\n",
    "        # Fit decoder\n",
    "        start_time = time.time()\n",
    "        outputs = []\n",
    "        psnr_run = []\n",
    "        \n",
    "        num = 5\n",
    "        for run_index in range(num):\n",
    "            if run_index > 0:\n",
    "                del decoder,fitter,output #Just in case\n",
    "            print('Decoder ',run_index)\n",
    "            decoder = create_model_from_configuration(fit_model_configuration)\n",
    "            fitter = create_fitter_from_configuration(fit_model_configuration)\n",
    "                    \n",
    "            random_variants = np.array(generate_random_variants(number_of_hidden_channels, \n",
    "                                                    input_size= (3*input_size[0],3*input_size[1]), # Check this out!\n",
    "                                                    blur = blur))\n",
    "        \n",
    "            noise_batch = divide_samples_into_patches(random_variants, (input_size[0],input_size[1]))\n",
    "            test_gauss_input = torch.from_numpy(noise_batch).type(dtype)\n",
    "\n",
    "            fitter(decoder, target_tensor, test_gauss_input)\n",
    "\n",
    "            output = decoder(test_gauss_input).detach().type(dtype)\n",
    "            outputs.append(output)\n",
    "            \n",
    "            print()\n",
    "                        \n",
    "            param_num = np.sum([weight.numel() for weight in decoder.parameters()])\n",
    "            print(f'Number of Parameters: {param_num}')\n",
    "            print()\n",
    "            \n",
    "            psnr_run.append(psnr(target_tensor, output).item())\n",
    "            \n",
    "            print('PSNR: ', psnr(target_tensor, output).item())\n",
    "            print()\n",
    "        \n",
    "        # Compute averaged output and PSNR variance to check for stability\n",
    "        psnr_variance=0\n",
    "        psnr_variance = np.var(psnr_run)\n",
    "        print('PSNR Variance among decoders: ',psnr_variance)\n",
    "        if run_index > 0:\n",
    "            del psnr_run\n",
    "        averaged_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "        current_psnr = psnr(target_tensor, averaged_output).item()\n",
    "        current_ssim = ssim(target_tensor, averaged_output).item()\n",
    "        #current_mssim = multi_scale_ssim(target_tensor, averaged_output).item()\n",
    "        current_vif = vif_p(target_tensor, averaged_output, sigma_n_sq=np.mean(original_image)).item()\n",
    "        \n",
    "        print('Ensemble metrics:')\n",
    "        print('PSNR: ', current_psnr)\n",
    "        print('SSIM: ', current_ssim)\n",
    "        #print('MS-SSIM: ', current_mssim)\n",
    "        print('VIF: ', current_vif)\n",
    "\n",
    "        print()\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Processing time: {execution_time} seconds\")\n",
    "        print()\n",
    "        print('-------------------------------------------------------------------------------------')\n",
    "\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([count, fit_model_configuration.number_of_hidden_channels, fit_model_configuration.input_shape,\n",
    "                         fit_model_configuration.number_of_layers, fit_model_configuration.number_of_iterations,\n",
    "                         current_psnr,psnr_variance, current_ssim, current_vif, param_num, execution_time])\n",
    "\n",
    "        # Check if current configuration has the best metric and update the corresponding best value\n",
    "        if current_psnr > best_psnr:\n",
    "            best_psnr = current_psnr\n",
    "            best_psnr_configuration = config\n",
    "\n",
    "        if current_ssim > best_ssim:\n",
    "            best_ssim = current_ssim\n",
    "            best_ssim_configuration = config\n",
    "\n",
    "        #if current_mssim > best_mssim:\n",
    "        #    best_mssim = current_mssim\n",
    "        #    best_mssim_configuration = config\n",
    "\n",
    "        if current_vif > best_vif:\n",
    "            best_vif = current_vif\n",
    "            best_vif_configuration = config\n",
    "\n",
    "# Print best configuration and metrics\n",
    "\n",
    "print('Best PSNR:', best_psnr)\n",
    "print('Best PSNR Configuration:', best_psnr_configuration)\n",
    "print('Best SSIM:', best_ssim)\n",
    "print('Best SSIM Configuration:', best_ssim_configuration)\n",
    "#print('Best MS-SSIM:', best_mssim)\n",
    "#print('Best MS-SIM Configuration:', best_mssim_configuration)\n",
    "print('Best VIF:', best_vif)\n",
    "print('Best VIF Configuration:', best_vif_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68100b8b-ded1-454e-af75-ef590b74e225",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grid Search Fitting Patches with 5 Decoders (Blur=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f2afe-9902-4613-ab0d-e56db51b19fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define target image\n",
    "if 'original_image' in locals():\n",
    "    del original_image\n",
    "\n",
    "if 'target_tensor' in locals():\n",
    "    del target_tensor\n",
    "# Original image must be (H,W,C)\n",
    "# Target tensor must be (Batch_size, C, H, W)\n",
    "\n",
    "\n",
    "sliced =  extract_MRI_slice('test_data/100206_MPR1.nii.gz')\n",
    "sliced_patches = divide_into_patches(sliced, (sliced.shape[0]//3, sliced.shape[1]//3))\n",
    "original_image = sliced_patches[0,0]\n",
    "batch_np = np.array(list(sliced_patches.values())).squeeze()[:,None,:,:]\n",
    "target_tensor = torch.from_numpy(batch_np).type(dtype)\n",
    "\n",
    "print('Original image shape: ', original_image.shape)\n",
    "print('Target tensor shape: ', target_tensor.shape)\n",
    "print()\n",
    "\n",
    "# Define candidate values for each parameter\n",
    "fit_model_configuration.image_dimensions = [target_tensor.shape[2], target_tensor.shape[3], target_tensor.shape[1]]\n",
    "number_of_hidden_channels_values = [8, 16, 32, 64, int(64*1.5), 64*2, 64*3]\n",
    "input_shape_values = [[original_image.shape[0] // 2**5, original_image.shape[1] // 2**5],\n",
    "                      [original_image.shape[0] // 2**4, original_image.shape[1] // 2**4],\n",
    "                      [original_image.shape[0] // 2**3, original_image.shape[1] // 2**3]]\n",
    "number_of_layers_values = [4, 5, 6]\n",
    "number_of_iterations_values = [2000, 5000, 7000]\n",
    "\n",
    "# Initialize variables\n",
    "best_psnr = 0\n",
    "best_ssim = 0\n",
    "best_mssim = 0\n",
    "best_vif = 0\n",
    "\n",
    "best_psnr_configuration = None\n",
    "best_ssim_configuration = None\n",
    "best_mssim_configuration = None\n",
    "best_vif_configuration = None\n",
    "\n",
    "with open('grid_search_results_patches_9_5DD_blur_3.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Run', 'Number_of_hidden_channels', 'Input_shape', 'Number_of_layers', 'Number_of_iterations',\n",
    "                     'PSNR','PSNR Variance', 'SSIM', 'VIF', 'Number_of_parameters', 'Execution_time'])\n",
    "\n",
    "    for count, config in enumerate(itertools.product(number_of_hidden_channels_values, input_shape_values,\n",
    "                                                     number_of_layers_values, number_of_iterations_values)):\n",
    "        \n",
    "                \n",
    "        decoder = None\n",
    "        fitter = None\n",
    "        output = None\n",
    "\n",
    "        fit_model_configuration.number_of_hidden_channels = config[0]\n",
    "        fit_model_configuration.input_shape = config[1]\n",
    "        fit_model_configuration.number_of_layers = config[2]\n",
    "        fit_model_configuration.number_of_iterations = config[3]\n",
    "\n",
    "        print(f'Run {count}:')\n",
    "        print('number_of_hidden_channels:', fit_model_configuration.number_of_hidden_channels)\n",
    "        print('input_shape:', fit_model_configuration.input_shape)\n",
    "        print('number_of_layers:', fit_model_configuration.number_of_layers)\n",
    "        print('number_of_iterations:', fit_model_configuration.number_of_iterations)\n",
    "        print()\n",
    "\n",
    "        number_of_hidden_channels = fit_model_configuration.number_of_hidden_channels\n",
    "        input_size = fit_model_configuration.input_shape\n",
    "        blur = 3\n",
    "\n",
    "        # Fit decoder\n",
    "        start_time = time.time()\n",
    "        outputs = []\n",
    "        psnr_run = []\n",
    "        \n",
    "        num = 5\n",
    "        for run_index in range(num):\n",
    "            if run_index > 0:\n",
    "                del decoder,fitter,output #Just in case\n",
    "            print('Decoder ',run_index)\n",
    "            decoder = create_model_from_configuration(fit_model_configuration)\n",
    "            fitter = create_fitter_from_configuration(fit_model_configuration)\n",
    "                    \n",
    "            random_variants = np.array(generate_random_variants(number_of_hidden_channels, \n",
    "                                                    input_size= (3*input_size[0],3*input_size[1]), # Check this out!\n",
    "                                                    blur = blur))\n",
    "        \n",
    "            noise_batch = divide_samples_into_patches(random_variants, (input_size[0],input_size[1]))\n",
    "            test_gauss_input = torch.from_numpy(noise_batch).type(dtype)\n",
    "\n",
    "            fitter(decoder, target_tensor, test_gauss_input)\n",
    "\n",
    "            output = decoder(test_gauss_input).detach().type(dtype)\n",
    "            outputs.append(output)\n",
    "            \n",
    "            print()\n",
    "                        \n",
    "            param_num = np.sum([weight.numel() for weight in decoder.parameters()])\n",
    "            print(f'Number of Parameters: {param_num}')\n",
    "            print()\n",
    "            \n",
    "            psnr_run.append(psnr(target_tensor, output).item())\n",
    "            \n",
    "            print('PSNR: ', psnr(target_tensor, output).item())\n",
    "            print()\n",
    "        \n",
    "        # Compute averaged output and PSNR variance to check for stability\n",
    "        psnr_variance=0\n",
    "        psnr_variance = np.var(psnr_run)\n",
    "        print('PSNR Variance among decoders: ',psnr_variance)\n",
    "        if run_index > 0:\n",
    "            del psnr_run\n",
    "        averaged_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "        current_psnr = psnr(target_tensor, averaged_output).item()\n",
    "        current_ssim = ssim(target_tensor, averaged_output).item()\n",
    "        #current_mssim = multi_scale_ssim(target_tensor, averaged_output).item()\n",
    "        current_vif = vif_p(target_tensor, averaged_output, sigma_n_sq=np.mean(original_image)).item()\n",
    "        \n",
    "        print('Ensemble metrics:')\n",
    "        print('PSNR: ', current_psnr)\n",
    "        print('SSIM: ', current_ssim)\n",
    "        #print('MS-SSIM: ', current_mssim)\n",
    "        print('VIF: ', current_vif)\n",
    "\n",
    "        print()\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Processing time: {execution_time} seconds\")\n",
    "        print()\n",
    "        print('-------------------------------------------------------------------------------------')\n",
    "\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([count, fit_model_configuration.number_of_hidden_channels, fit_model_configuration.input_shape,\n",
    "                         fit_model_configuration.number_of_layers, fit_model_configuration.number_of_iterations,\n",
    "                         current_psnr,psnr_variance, current_ssim, current_vif, param_num, execution_time])\n",
    "\n",
    "        # Check if current configuration has the best metric and update the corresponding best value\n",
    "        if current_psnr > best_psnr:\n",
    "            best_psnr = current_psnr\n",
    "            best_psnr_configuration = config\n",
    "\n",
    "        if current_ssim > best_ssim:\n",
    "            best_ssim = current_ssim\n",
    "            best_ssim_configuration = config\n",
    "\n",
    "        #if current_mssim > best_mssim:\n",
    "        #    best_mssim = current_mssim\n",
    "        #    best_mssim_configuration = config\n",
    "\n",
    "        if current_vif > best_vif:\n",
    "            best_vif = current_vif\n",
    "            best_vif_configuration = config\n",
    "\n",
    "# Print best configuration and metrics\n",
    "\n",
    "print('Best PSNR:', best_psnr)\n",
    "print('Best PSNR Configuration:', best_psnr_configuration)\n",
    "print('Best SSIM:', best_ssim)\n",
    "print('Best SSIM Configuration:', best_ssim_configuration)\n",
    "#print('Best MS-SSIM:', best_mssim)\n",
    "#print('Best MS-SIM Configuration:', best_mssim_configuration)\n",
    "print('Best VIF:', best_vif)\n",
    "print('Best VIF Configuration:', best_vif_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9d75c-1086-46d6-8347-f75d67c779d3",
   "metadata": {},
   "source": [
    "# Grid Search Fitting Patches with 5 Decoders (Perlin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97372e48-9ca4-4a9f-91b5-c0807bdfa182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caution, not perfect division\n",
      "Original image shape:  (85, 106, 1)\n",
      "Target tensor shape:  torch.Size([9, 1, 85, 106])\n",
      "\n",
      "Run 0:\n",
      "number_of_hidden_channels: 8\n",
      "input_shape: [2, 3]\n",
      "number_of_layers: 4\n",
      "number_of_iterations: 2000\n",
      "\n",
      "Decoder  0\n",
      "Step: 02000, Loss: 0.008410, Minimum Loss at: 1807 with 0.008400\n",
      "Number of Parameters: 328\n",
      "\n",
      "PSNR:  21.239290237426758\n",
      "\n",
      "Decoder  1\n",
      "Step: 00150, Loss: 0.015043, Minimum Loss at: 144 with 0.014779\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m decoder \u001b[38;5;241m=\u001b[39m create_model_from_configuration(fit_model_configuration)\n\u001b[1;32m     86\u001b[0m fitter \u001b[38;5;241m=\u001b[39m create_fitter_from_configuration(fit_model_configuration)\n\u001b[0;32m---> 88\u001b[0m \u001b[43mfitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplex_noise_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m output \u001b[38;5;241m=\u001b[39m decoder(simplex_noise_tensor)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtype(dtype)\n\u001b[1;32m     91\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "Cell \u001b[0;32mIn[10], line 82\u001b[0m, in \u001b[0;36mFitter.__call__\u001b[0;34m(self, model, original_image, fixed_net_input, log_prefix, loss_mask)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prefix \u001b[38;5;241m=\u001b[39m log_prefix\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 93\u001b[0m, in \u001b[0;36mFitter.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_loss_metrics_and_best_model(loss, output)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_log():\n",
      "File \u001b[0;32m/itet-stor/pblasco/net_scratch/miniconda3/envs/DD/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:67\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     66\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/pblasco/net_scratch/miniconda3/envs/DD/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/itet-stor/pblasco/net_scratch/miniconda3/envs/DD/lib/python3.8/site-packages/torch/optim/adam.py:66\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m---> 66\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m     69\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[10], line 87\u001b[0m, in \u001b[0;36mFitter.fit.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m():\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_net_input)\n\u001b[1;32m     89\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoisy_image, output)\n",
      "File \u001b[0;32m/itet-stor/pblasco/net_scratch/miniconda3/envs/DD/lib/python3.8/site-packages/torch/optim/optimizer.py:191\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    189\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdetach_()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    192\u001b[0m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n",
      "File \u001b[0;32m/itet-stor/pblasco/net_scratch/miniconda3/envs/DD/lib/python3.8/site-packages/torch/tensor.py:942\u001b[0m, in \u001b[0;36mTensor.grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03mThis attribute is ``None`` by default and becomes a Tensor the first time a call to\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;124;03m:func:`backward` computes gradients for ``self``.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03mThe attribute will then contain the gradients computed and future calls to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03m:func:`backward` will accumulate (add) gradients into it.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    941\u001b[0m relevant_args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m,)\n\u001b[0;32m--> 942\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_torch_function, handle_torch_function\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Tensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;66;03m# TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m, relevant_args, \u001b[38;5;28mself\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Original image must be (H,W,C)\n",
    "# Target tensor must be (Batch_size, C, H, W)\n",
    "sliced =  extract_MRI_slice('test_data/100206_MPR1.nii.gz')\n",
    "sliced_patches = divide_into_patches(sliced, (sliced.shape[0]//3, sliced.shape[1]//3))\n",
    "original_image = sliced_patches[0,0]\n",
    "batch_np = np.array(list(sliced_patches.values())).squeeze()[:,None,:,:]\n",
    "target_tensor = torch.from_numpy(batch_np).type(dtype)\n",
    "\n",
    "print('Original image shape: ', original_image.shape)\n",
    "print('Target tensor shape: ', target_tensor.shape)\n",
    "print()\n",
    "\n",
    "# Define candidate values for each parameter\n",
    "fit_model_configuration.image_dimensions = [target_tensor.shape[2], target_tensor.shape[3], target_tensor.shape[1]]\n",
    "\n",
    "number_of_hidden_channels_values = [8, 16, 32, 64, int(64*1.5), 64*2, 64*3]\n",
    "input_shape_values = [[original_image.shape[0] // 2**5, original_image.shape[1] // 2**5],\n",
    "                      [original_image.shape[0] // 2**4, original_image.shape[1] // 2**4],\n",
    "                      [original_image.shape[0] // 2**3, original_image.shape[1] // 2**3]]\n",
    "                        \n",
    "\n",
    "# [original_image.shape[0] // 5, original_image.shape[1] // 5]]\n",
    "\n",
    "number_of_layers_values = [4, 5, 6]\n",
    "number_of_iterations_values = [2000, 5000, 7000]\n",
    "\n",
    "# Initialize variables\n",
    "best_psnr = 0\n",
    "best_ssim = 0\n",
    "best_mssim = 0\n",
    "best_vif = 0\n",
    "\n",
    "best_psnr_configuration = None\n",
    "best_ssim_configuration = None\n",
    "best_mssim_configuration = None\n",
    "best_vif_configuration = None\n",
    "\n",
    "with open('grid_search_results_patches_9_5DD_perlin.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Run', 'Number_of_hidden_channels', 'Input_shape', 'Number_of_layers', 'Number_of_iterations',\n",
    "                     'PSNR', 'SSIM', 'VIF', 'Number_of_parameters', 'Execution_time'])\n",
    "\n",
    "    for count, config in enumerate(itertools.product(number_of_hidden_channels_values, input_shape_values,\n",
    "                                                     number_of_layers_values, number_of_iterations_values)):\n",
    "        \n",
    "                \n",
    "        decoder = None\n",
    "        fitter = None\n",
    "        output = None\n",
    "\n",
    "        fit_model_configuration.number_of_hidden_channels = config[0]\n",
    "        fit_model_configuration.input_shape = config[1]\n",
    "        fit_model_configuration.number_of_layers = config[2]\n",
    "        fit_model_configuration.number_of_iterations = config[3]\n",
    "\n",
    "        print(f'Run {count}:')\n",
    "        print('number_of_hidden_channels:', fit_model_configuration.number_of_hidden_channels)\n",
    "        print('input_shape:', fit_model_configuration.input_shape)\n",
    "        print('number_of_layers:', fit_model_configuration.number_of_layers)\n",
    "        print('number_of_iterations:', fit_model_configuration.number_of_iterations)\n",
    "        print()\n",
    "\n",
    "        # Fit decoder\n",
    "        start_time = time.time()\n",
    "        outputs = []\n",
    "        psnr_run = []\n",
    "        \n",
    "        num = 5\n",
    "        for run_index in range(num):\n",
    "            if run_index > 0:\n",
    "                del decoder,fitter,output #Just in case\n",
    "            print('Decoder ',run_index)\n",
    "            \n",
    "            number_of_hidden_channels = fit_model_configuration.number_of_hidden_channels\n",
    "            input_size = fit_model_configuration.input_shape\n",
    "        \n",
    "            simplex_noise = generate_simplex_noise_patches(size = (3*input_size[0], 3*input_size[1]),\n",
    "                                               number_of_channels = number_of_hidden_channels, \n",
    "                                               seed = np.random.randint(1, 10**3))\n",
    "        \n",
    "            noise_batch = divide_samples_into_patches(simplex_noise,(input_size[0],input_size[1]))\n",
    "            simplex_noise_tensor = torch.from_numpy(noise_batch).type(dtype)\n",
    "            \n",
    "            \n",
    "            decoder = create_model_from_configuration(fit_model_configuration)\n",
    "            fitter = create_fitter_from_configuration(fit_model_configuration)\n",
    "\n",
    "            fitter(decoder, target_tensor, simplex_noise_tensor)\n",
    "\n",
    "            output = decoder(simplex_noise_tensor).detach().type(dtype)\n",
    "            outputs.append(output)\n",
    "            \n",
    "            print()\n",
    "                        \n",
    "            param_num = np.sum([weight.numel() for weight in decoder.parameters()])\n",
    "            print(f'Number of Parameters: {param_num}')\n",
    "            print()\n",
    "            \n",
    "            psnr_run.append(psnr(target_tensor, output).item())\n",
    "            \n",
    "            print('PSNR: ', psnr(target_tensor, output).item())\n",
    "            print()\n",
    "        \n",
    "        # Compute averaged output and PSNR variance to check for stability\n",
    "        psnr_variance=0\n",
    "        psnr_variance = np.var(psnr_run)\n",
    "        print('PSNR Variance among decoders: ',psnr_variance)\n",
    "        if run_index > 0:\n",
    "            del psnr_run\n",
    "        averaged_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "        current_psnr = psnr(target_tensor, averaged_output).item()\n",
    "        current_ssim = ssim(target_tensor, averaged_output).item()\n",
    "        #current_mssim = multi_scale_ssim(target_tensor, averaged_output).item()\n",
    "        current_vif = vif_p(target_tensor, averaged_output, sigma_n_sq=np.mean(original_image)).item()\n",
    "        \n",
    "        print('Ensemble metrics:')\n",
    "        print('PSNR: ', current_psnr)\n",
    "        print('SSIM: ', current_ssim)\n",
    "        #print('MS-SSIM: ', current_mssim)\n",
    "        print('VIF: ', current_vif)\n",
    "\n",
    "        print()\n",
    "\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Processing time: {execution_time} seconds\")\n",
    "        print()\n",
    "        print('-------------------------------------------------------------------------------------')\n",
    "\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([count, fit_model_configuration.number_of_hidden_channels, fit_model_configuration.input_shape,\n",
    "                         fit_model_configuration.number_of_layers, fit_model_configuration.number_of_iterations,\n",
    "                         current_psnr,psnr_variance, current_ssim, current_vif, param_num, execution_time])\n",
    "\n",
    "        # Check if current configuration has the best metric and update the corresponding best value\n",
    "        if current_psnr > best_psnr:\n",
    "            best_psnr = current_psnr\n",
    "            best_psnr_configuration = config\n",
    "\n",
    "        if current_ssim > best_ssim:\n",
    "            best_ssim = current_ssim\n",
    "            best_ssim_configuration = config\n",
    "\n",
    "        #if current_mssim > best_mssim:\n",
    "        #    best_mssim = current_mssim\n",
    "        #    best_mssim_configuration = config\n",
    "\n",
    "        if current_vif > best_vif:\n",
    "            best_vif = current_vif\n",
    "            best_vif_configuration = config\n",
    "\n",
    "# Print best configuration and metrics\n",
    "\n",
    "print('Best PSNR:', best_psnr)\n",
    "print('Best PSNR Configuration:', best_psnr_configuration)\n",
    "print('Best SSIM:', best_ssim)\n",
    "print('Best SSIM Configuration:', best_ssim_configuration)\n",
    "#print('Best MS-SSIM:', best_mssim)\n",
    "#print('Best MS-SIM Configuration:', best_mssim_configuration)\n",
    "print('Best VIF:', best_vif)\n",
    "print('Best VIF Configuration:', best_vif_configuration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
